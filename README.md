# Hi there, I'm [Your Name]! ðŸ‘‹

## ðŸš€ Projects

- [Time Series Data Prediction]

    [State Space Model](https://github.com/dharvpat/time-series-data-prediction-SSM): Using State Space Models to perform Statistical Approximation of Time Series data. This methodology works extremely well when the cause of change in the data is something unpredictable like People, Supply/Demand, Weather Patterns, etc.

    [SVM](https://github.com/dharvpat/SVM-Time-series-data-prediction): Using Support Vector Machines to perform Numerical Approximation of Time Series data. This methodology works extremely well when the cause of change in the data is something predictable like Nature or any other physical processes.

    [LSTM](https://github.com/dharvpat/LSTM_stock_prediciton): Using Long Short Term Memory RNNs to perform Deep Learning Modeling of Time Series data. This methodology works extremely well when the cause of change in the data is a Recurrent Process. This is particularly accurate for stock data as any changes in the prices recursively changes the price further.


- [Papers in code]

    [Deep Reinforcement Learning](https://github.com/dharvpat/Papers-in-code/tree/main/A%20survey%20on%20deep%20reinforcement%20learning): Reinforcement Learning using both DQN and A3C methodologies. We explore Transfer Learning as well as Trust Region Policy Optimization and Proximal Policy Optimization.

    [Attention is all you need (Transformers)](https://github.com/dharvpat/Papers-in-code/tree/main/Attention%20is%20all%20you%20need): Attention is all you need is a revolutionary paper that changed everything about AI and kicked it into high gear. We discuss the various uses of Transformers as well as NLP implementations of Transformers.

    [Contrastive Learning](https://github.com/dharvpat/Papers-in-code/tree/main/Contrastive%20Learning): Contrastive Learning explores self-supervised Learning in terms of architecture and Loss functions.

    [Diffusion](https://github.com/dharvpat/Papers-in-code/tree/main/DiffEnc%3A%20Variational%20Diffusion%20with%20a%20Learned%20Encoder): Diffusion models with a learned encoder for image generation and other Generative purposes.

    [RCNN](https://github.com/dharvpat/Papers-in-code/tree/main/Faster%20RCNN): CNN mopdels that are quicker than Traditional CNNs due to different processing pipeline and astonishingly well at accuracy comparedx to traditional CNNs.

- [Variatonal AutoEncoder for Image Generation](https://github.com/dharvpat/VAE): Variational AutoEncoders are a form of Generative Advesarial Networks which are trained to recreate certain images depending on the image set we train on. A more complicated Encoder Decoder Network

- 

## ðŸ’¼ Skills

- **Languages:** Python, JavaScript, C++
- **Frameworks:** React, Node.js, Django

## ðŸ“« Get in Touch

- [LinkedIn](https://www.linkedin.com/in/yourprofile)
- [Email](mailto:youremail@example.com)
