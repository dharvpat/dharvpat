# Hi there, I'm [Your Name]! ðŸ‘‹

## ðŸš€ Projects

### Time Series Data Prediction

[State Space Model](https://github.com/dharvpat/time-series-data-prediction-SSM): Using State Space Models to perform Statistical Approximation of Time Series data. This methodology works extremely well when the cause of change in the data is something unpredictable like People, Supply/Demand, Weather Patterns, etc.

[SVM](https://github.com/dharvpat/SVM-Time-series-data-prediction): Using Support Vector Machines to perform Numerical Approximation of Time Series data. This methodology works extremely well when the cause of change in the data is something predictable like Nature or any other physical processes.

[LSTM](https://github.com/dharvpat/LSTM_stock_prediciton): Using Long Short Term Memory RNNs to perform Deep Learning Modeling of Time Series data. This methodology works extremely well when the cause of change in the data is a Recurrent Process. This is particularly accurate for stock data as any changes in the prices recursively changes the price further.


### Papers in code

[Deep Reinforcement Learning](https://github.com/dharvpat/Papers-in-code/tree/main/A%20survey%20on%20deep%20reinforcement%20learning): Reinforcement Learning using both DQN and A3C methodologies. We explore Transfer Learning as well as Trust Region Policy Optimization and Proximal Policy Optimization.

[Attention is all you need (Transformers)](https://github.com/dharvpat/Papers-in-code/tree/main/Attention%20is%20all%20you%20need): Attention is all you need is a revolutionary paper that changed everything about AI and kicked it into high gear. We discuss the various uses of Transformers as well as NLP implementations of Transformers.

[Contrastive Learning](https://github.com/dharvpat/Papers-in-code/tree/main/Contrastive%20Learning): Contrastive Learning explores self-supervised Learning in terms of architecture and Loss functions.

[Diffusion](https://github.com/dharvpat/Papers-in-code/tree/main/DiffEnc%3A%20Variational%20Diffusion%20with%20a%20Learned%20Encoder): Diffusion models with a learned encoder for image generation and other Generative purposes.

[RCNN](https://github.com/dharvpat/Papers-in-code/tree/main/Faster%20RCNN): CNN mopdels that are quicker than Traditional CNNs due to different processing pipeline and astonishingly well at accuracy comparedx to traditional CNNs.

### Generation Networks

[Variatonal AutoEncoder for Image Generation](https://github.com/dharvpat/VAE): Variational AutoEncoders are a form of Generative Advesarial Networks which are trained to recreate certain images depending on the image set we train on. A more complicated Encoder Decoder Network

[Deep Convolution Generative Adversarial Netowrks](https://github.com/dharvpat/MNIST-handwritten-experiments): A modified Encoder-Decoder Network that can ingest noise and output handwritten numbers

### Natural Language Processing

[Natural Language Understanding](https://github.com/dharvpat/NLP): A repository of work on Natural Language understanding using well-established statistical methods.

[Sentiment Analysis](https://github.com/dharvpat/sentiment-analysis): A sentiment analysis model using Convolution Neural Networks. This involved interesting challenges in using words and phrases as vectors

### [Noise Cancellation using Neural Networks](https://github.com/dharvpat/Denoising-AutoEncoder): We attempt to build a real-time Model for Noise Cancellation. The project started on the basis of an AutoEncoder capable of removing noise from an input stream of data. However, due to challenges regarding the model architecture, we switched to a Unet1D model that can achieve comparable results by using significantly less wall time. 

### [Image Compression using Neural Networks](https://github.com/dharvpat/image): Image Compression using Neural Networks, [IN PROGRESS]

## ðŸ’¼ Skills

- **Languages:** Python, CUDA, C++, JavaScript
- **Frameworks:** Tensorflow, pytorch

## ðŸ“« Get in Touch

- [LinkedIn](https://www.linkedin.com/in/dharv-p-67469b140/)
- [Email](mailto:dharv.patel@gmail.com)
